{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho 2: Redes Convolucionais\n",
    "\n",
    "### Alunos\n",
    "- **Arthur de Sá Antero - 212006577**\n",
    "- **Arthur Mota Furtado - 200014935**\n",
    "\n",
    "Neste trabalho, iremos aplicar algumas redes convolucionais para a tarefa de classificação de imagens. Como base de treinamento, usaremos um conjunto de imagens de flores, que serão classificadas em 5 classes: tulipa, girassol, rosa, dente-de-leão e margarida.\n",
    "\n",
    "Inicialmente treinaremos um simples rede MLP com uma camada oculta, e depois iremos testar diferentes tipos de redes convolucionais pré-treinadas, que colocaremos antes das camadas densas da rede MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados\n",
    "\n",
    "Aqui carregamos os dados diretamente da biblioteca tensorflow_datasets, e normalizamos os dados para o formato que o modelo espera. (imagens de 150x150, com 3 canais, float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 10:49:31.667909: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-22 10:49:31.782603: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2024-12-22 10:49:31.782884: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2611200000 Hz\n",
      "2024-12-22 10:49:32.729927: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3488249856 exceeds 10% of free system memory.\n",
      "2024-12-22 10:49:34.207193: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 932238720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2569, 150, 150, 3)\n",
      "(1101, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "## Loading images and labels\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[70%:]\"], ## Train test split\n",
    "    batch_size=-1,\n",
    "    as_supervised=True, # Include labels\n",
    ")\n",
    "## Resizing images\n",
    "train_ds = tf.image.resize(train_ds, (150, 150))\n",
    "test_ds = tf.image.resize(test_ds, (150, 150))\n",
    "## Transforming labels to correct format\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "print (train_ds.shape)\n",
    "print (test_ds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaração do modelo MLP\n",
    "\n",
    "Aqui temos os a declaração do modelo MLP, que é um modelo de rede neural multi-camada, com uma função de ativação sigmoide e uma função de regressão linear. Ele utilizará duas camadas de 128 neurônios, sendo ambas ativadas com a função relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_wNymRBKEBZA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 3s 47ms/step - loss: 509.4144 - accuracy: 0.2533 - val_loss: 158.3735 - val_accuracy: 0.3521\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 127.3345 - accuracy: 0.3652 - val_loss: 64.5273 - val_accuracy: 0.3813\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 3s 47ms/step - loss: 49.8208 - accuracy: 0.3930 - val_loss: 34.9322 - val_accuracy: 0.3346\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 3s 49ms/step - loss: 27.6560 - accuracy: 0.4279 - val_loss: 27.3815 - val_accuracy: 0.3268\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 3s 48ms/step - loss: 18.5574 - accuracy: 0.4149 - val_loss: 23.0015 - val_accuracy: 0.2763\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 3s 48ms/step - loss: 13.0571 - accuracy: 0.4567 - val_loss: 12.3043 - val_accuracy: 0.3735\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 3s 47ms/step - loss: 6.6835 - accuracy: 0.5025 - val_loss: 16.5219 - val_accuracy: 0.3521\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 453.4857 - accuracy: 0.3488\n",
      "Accuracy: 0.3487738370895386\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True)\n",
    "model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_ds, test_labels)\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao treinarmos o modelo, percebemos que a acurácia do modelo é muito abaixo de um valor aceitável, com o modelo errando na maioria dos casos. Isso dá pela natureza de lidar com imagens mais complexas, que modelos não convolucionais tem uma maior dificuldade em generalizar.\n",
    "\n",
    "Para resolver esse problema, utilizaremos uma outra rede convolucional já pré-treinada, que é o VGG16, como base para o modelo. Essa rede é treinada para classificar imagens de alta resolução, e é capaz de generalizar bem para imagens de baixa resolução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "## Preprocessing input\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois da saída da rede VGG16, treinaremos um topo uma nova camada MLP bem simples, que possa classificar imagens de maneira mais precisa. Como já temos muito processamento dentro da rede VGG16, podemos usar camadas com menos neurônios e com menos parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O3aEGoNJGCDa"
   },
   "outputs": [],
   "source": [
    "convolutional_model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(5, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tMRPJCvGuMD",
    "outputId": "6d8386df-0c93-4e2f-e5cb-f1bce1abb535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 126s 2s/step - loss: 2.9836 - accuracy: 0.4560 - val_loss: 1.2050 - val_accuracy: 0.5817\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 131s 2s/step - loss: 0.6830 - accuracy: 0.7455 - val_loss: 1.1780 - val_accuracy: 0.6206\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 131s 2s/step - loss: 0.4452 - accuracy: 0.8288 - val_loss: 1.1863 - val_accuracy: 0.6420\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 132s 2s/step - loss: 0.2822 - accuracy: 0.9120 - val_loss: 1.1315 - val_accuracy: 0.6887\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 132s 2s/step - loss: 0.1585 - accuracy: 0.9485 - val_loss: 1.2175 - val_accuracy: 0.6868\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 133s 2s/step - loss: 0.1115 - accuracy: 0.9609 - val_loss: 1.2734 - val_accuracy: 0.6887\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 134s 2s/step - loss: 0.0809 - accuracy: 0.9724 - val_loss: 1.3385 - val_accuracy: 0.6965\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 135s 2s/step - loss: 0.0462 - accuracy: 0.9864 - val_loss: 1.4639 - val_accuracy: 0.6926\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 135s 2s/step - loss: 0.0285 - accuracy: 0.9959 - val_loss: 1.5073 - val_accuracy: 0.6868\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 135s 2s/step - loss: 0.0145 - accuracy: 0.9984 - val_loss: 1.5683 - val_accuracy: 0.6848\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 135s 2s/step - loss: 0.0069 - accuracy: 0.9998 - val_loss: 1.6251 - val_accuracy: 0.6848\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 135s 2s/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 1.6540 - val_accuracy: 0.6907\n",
      "35/35 [==============================] - 58s 2s/step - loss: 1.5305 - accuracy: 0.6785\n",
      "Accuracy: 0.6784741282463074\n"
     ]
    }
   ],
   "source": [
    "convolutional_model.compile(\n",
    "optimizer='adam',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'],\n",
    ")\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True)\n",
    "convolutional_model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = convolutional_model.evaluate(test_ds, test_labels)\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, mesmo com a rede VGG16, o resultado não é muito bom. O que ressalta o quão difícil é o dataset de treinamento, pois o dataset tem muito dados de treinamento, e as imagens em si são muito diferentes. Para melhorarmos o resultados, tentaremos usar uma outra rede neural, e outras técnicas de melhoria do aprendizado de máquina, para evitar pontos ótimos locais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 4s 0us/step\n",
      "(None, 3, 3, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "\n",
    "## Loading ResNet50 model\n",
    "base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "## Preprocessing input\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)\n",
    "\n",
    "print(base_model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "convolutional_model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(5, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 25s 357ms/step - loss: 3.7134 - accuracy: 0.3030 - val_loss: 1.4412 - val_accuracy: 0.4572\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 24s 371ms/step - loss: 1.1431 - accuracy: 0.5615 - val_loss: 1.3455 - val_accuracy: 0.4825\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 24s 370ms/step - loss: 1.0314 - accuracy: 0.5924 - val_loss: 1.2547 - val_accuracy: 0.5370\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 24s 370ms/step - loss: 0.8300 - accuracy: 0.6746 - val_loss: 1.2195 - val_accuracy: 0.5525\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 24s 368ms/step - loss: 0.8002 - accuracy: 0.6942 - val_loss: 1.2791 - val_accuracy: 0.5467\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 24s 370ms/step - loss: 0.6708 - accuracy: 0.7547 - val_loss: 1.2659 - val_accuracy: 0.5564\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 24s 371ms/step - loss: 0.5861 - accuracy: 0.7864 - val_loss: 1.2864 - val_accuracy: 0.5623\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 24s 378ms/step - loss: 0.5470 - accuracy: 0.8122 - val_loss: 1.2577 - val_accuracy: 0.5525\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 25s 389ms/step - loss: 0.4345 - accuracy: 0.8710 - val_loss: 1.6132 - val_accuracy: 0.4883\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 0.4417 - accuracy: 0.8412 - val_loss: 1.4628 - val_accuracy: 0.5778\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 0.3019 - accuracy: 0.9051 - val_loss: 1.5412 - val_accuracy: 0.5700\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 24s 372ms/step - loss: 0.2661 - accuracy: 0.9246 - val_loss: 1.6432 - val_accuracy: 0.5467\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 24s 372ms/step - loss: 0.2331 - accuracy: 0.9251 - val_loss: 1.5481 - val_accuracy: 0.5700\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 24s 373ms/step - loss: 0.1781 - accuracy: 0.9487 - val_loss: 1.5963 - val_accuracy: 0.5798\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 24s 373ms/step - loss: 0.1537 - accuracy: 0.9606 - val_loss: 1.8578 - val_accuracy: 0.5467\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 24s 373ms/step - loss: 0.1445 - accuracy: 0.9551 - val_loss: 1.9375 - val_accuracy: 0.5370\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 24s 373ms/step - loss: 0.1216 - accuracy: 0.9720 - val_loss: 1.7979 - val_accuracy: 0.5798\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 0.0818 - accuracy: 0.9821 - val_loss: 1.9028 - val_accuracy: 0.5837\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 24s 376ms/step - loss: 0.0656 - accuracy: 0.9863 - val_loss: 1.8889 - val_accuracy: 0.5720\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 0.0750 - accuracy: 0.9828 - val_loss: 2.0287 - val_accuracy: 0.5720\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 24s 376ms/step - loss: 0.0725 - accuracy: 0.9813 - val_loss: 2.0986 - val_accuracy: 0.5661\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 24s 377ms/step - loss: 0.0396 - accuracy: 0.9948 - val_loss: 2.4164 - val_accuracy: 0.5156\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 0.1087 - accuracy: 0.9663 - val_loss: 3.1144 - val_accuracy: 0.4747\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 24s 377ms/step - loss: 0.4399 - accuracy: 0.8658 - val_loss: 2.3628 - val_accuracy: 0.5233\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 25s 378ms/step - loss: 0.1847 - accuracy: 0.9247 - val_loss: 2.3874 - val_accuracy: 0.5311\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 25s 383ms/step - loss: 0.0799 - accuracy: 0.9747 - val_loss: 2.3092 - val_accuracy: 0.5720\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 0.0338 - accuracy: 0.9932 - val_loss: 2.4867 - val_accuracy: 0.5428\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 26s 395ms/step - loss: 0.0170 - accuracy: 0.9995 - val_loss: 2.4453 - val_accuracy: 0.5603\n",
      "35/35 [==============================] - 10s 299ms/step - loss: 2.6508 - accuracy: 0.4868\n",
      "Accuracy: 0.4868301451206207\n"
     ]
    }
   ],
   "source": [
    "convolutional_model.compile(\n",
    "optimizer='adam',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'],\n",
    ")\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True)\n",
    "convolutional_model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = convolutional_model.evaluate(test_ds, test_labels)\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver o resultado da rede Inception3 também não é satisfatório e ainda pior que da rede VGG16. Isso ressalta a importância de termos outros mecanismos de aprendizado a serem usados ao longo do treinamento da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o Mesmo modelo com regularização L1 e L2\n",
    "\n",
    "Uma ótima ferramenta para melhorar o trainamento de um modelo é a regularização L1 e L2. Estes se tratam de duas funções de regularização que adicionam um termo para a função de perda, o que facilita a encontrar um melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "convolutional_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),  # L1/L2 regularization\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),\n",
    "    layers.Dense(5, activation='softmax')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "65/65 [==============================] - 28s 402ms/step - loss: 4.1665 - accuracy: 0.3297 - val_loss: 2.4443 - val_accuracy: 0.5428\n",
      "Epoch 2/30\n",
      "65/65 [==============================] - 25s 383ms/step - loss: 2.1579 - accuracy: 0.5795 - val_loss: 2.1549 - val_accuracy: 0.4786\n",
      "Epoch 3/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 1.8310 - accuracy: 0.5792 - val_loss: 1.9116 - val_accuracy: 0.5233\n",
      "Epoch 4/30\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 1.5279 - accuracy: 0.6429 - val_loss: 1.9445 - val_accuracy: 0.5175\n",
      "Epoch 5/30\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 1.3743 - accuracy: 0.6775 - val_loss: 1.6578 - val_accuracy: 0.5603\n",
      "Epoch 6/30\n",
      "65/65 [==============================] - 25s 389ms/step - loss: 1.2151 - accuracy: 0.7304 - val_loss: 2.1159 - val_accuracy: 0.4728\n",
      "Epoch 7/30\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 1.1839 - accuracy: 0.7437 - val_loss: 1.8050 - val_accuracy: 0.5311\n",
      "Epoch 8/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 1.1546 - accuracy: 0.7358 - val_loss: 1.9298 - val_accuracy: 0.5409\n",
      "Epoch 9/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 1.1934 - accuracy: 0.7420 - val_loss: 1.6989 - val_accuracy: 0.5759\n",
      "Epoch 10/30\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 1.0290 - accuracy: 0.8080 - val_loss: 2.0624 - val_accuracy: 0.5311\n",
      "Epoch 11/30\n",
      "65/65 [==============================] - 25s 383ms/step - loss: 0.9967 - accuracy: 0.8137 - val_loss: 1.7620 - val_accuracy: 0.5525\n",
      "Epoch 12/30\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 0.8904 - accuracy: 0.8426 - val_loss: 2.2226 - val_accuracy: 0.4689\n",
      "Epoch 13/30\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 0.9323 - accuracy: 0.8364 - val_loss: 1.8466 - val_accuracy: 0.5525\n",
      "Epoch 14/30\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 0.9106 - accuracy: 0.8360 - val_loss: 1.8127 - val_accuracy: 0.5895\n",
      "Epoch 15/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 0.8016 - accuracy: 0.8962 - val_loss: 2.2891 - val_accuracy: 0.5253\n",
      "Epoch 16/30\n",
      "65/65 [==============================] - 24s 375ms/step - loss: 0.8134 - accuracy: 0.8782 - val_loss: 2.1857 - val_accuracy: 0.5272\n",
      "Epoch 17/30\n",
      "65/65 [==============================] - 24s 375ms/step - loss: 0.8278 - accuracy: 0.8809 - val_loss: 2.2187 - val_accuracy: 0.5136\n",
      "Epoch 18/30\n",
      "65/65 [==============================] - 24s 375ms/step - loss: 0.7918 - accuracy: 0.8793 - val_loss: 2.3476 - val_accuracy: 0.5078\n",
      "Epoch 19/30\n",
      "65/65 [==============================] - 24s 373ms/step - loss: 0.8585 - accuracy: 0.8669 - val_loss: 2.2125 - val_accuracy: 0.5467\n",
      "Epoch 20/30\n",
      "65/65 [==============================] - 24s 377ms/step - loss: 0.8181 - accuracy: 0.8892 - val_loss: 2.1872 - val_accuracy: 0.5409\n",
      "Epoch 21/30\n",
      "65/65 [==============================] - 24s 377ms/step - loss: 0.9004 - accuracy: 0.8487 - val_loss: 2.5259 - val_accuracy: 0.5078\n",
      "Epoch 22/30\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 0.8674 - accuracy: 0.8497 - val_loss: 2.5788 - val_accuracy: 0.5097\n",
      "Epoch 23/30\n",
      "65/65 [==============================] - 25s 383ms/step - loss: 0.7497 - accuracy: 0.9058 - val_loss: 2.1970 - val_accuracy: 0.5661\n",
      "Epoch 24/30\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 0.6564 - accuracy: 0.9423 - val_loss: 2.0831 - val_accuracy: 0.5681\n",
      "Epoch 25/30\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 0.6926 - accuracy: 0.9213 - val_loss: 2.2757 - val_accuracy: 0.5623\n",
      "Epoch 26/30\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 0.6279 - accuracy: 0.9428 - val_loss: 2.2007 - val_accuracy: 0.5700\n",
      "Epoch 27/30\n",
      "65/65 [==============================] - 25s 386ms/step - loss: 0.5201 - accuracy: 0.9778 - val_loss: 2.2557 - val_accuracy: 0.5564\n",
      "Epoch 28/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 0.5379 - accuracy: 0.9698 - val_loss: 2.3472 - val_accuracy: 0.5370\n",
      "Epoch 29/30\n",
      "65/65 [==============================] - 25s 378ms/step - loss: 0.7635 - accuracy: 0.8658 - val_loss: 3.1178 - val_accuracy: 0.4981\n",
      "Epoch 30/30\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 0.9885 - accuracy: 0.8606 - val_loss: 2.6539 - val_accuracy: 0.5136\n",
      "35/35 [==============================] - 10s 295ms/step - loss: 3.0512 - accuracy: 0.4959\n",
      "Accuracy: 0.4959128201007843\n"
     ]
    }
   ],
   "source": [
    "convolutional_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "convolutional_model.fit(train_ds, train_labels, epochs=30, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = convolutional_model.evaluate(test_ds, test_labels)\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, mesmo com as regularizações, não conseguimos melhorar o resultado. Isso aponta para que o modelo não está realmente ajustando bem o problema, e que para melhorar o resultado, é preciso melhorar os dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "convolutional_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),  # L1/L2 regularization\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),\n",
    "    layers.Dropout(0.05),\n",
    "    layers.Dense(5, activation='softmax')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao Adicionar camadas de Dropout no modelo permitimos a retirada de alguns pesos que estão em ótimos locais, andarmos em direção a um ótimo global, o que deve acarretar em uma melhor performance no final do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "65/65 [==============================] - 27s 383ms/step - loss: 5.2109 - accuracy: 0.2545 - val_loss: 2.7968 - val_accuracy: 0.4261\n",
      "Epoch 2/30\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 2.6148 - accuracy: 0.4404 - val_loss: 2.2362 - val_accuracy: 0.4805\n",
      "Epoch 3/30\n",
      "65/65 [==============================] - 24s 378ms/step - loss: 2.1034 - accuracy: 0.5043 - val_loss: 1.9076 - val_accuracy: 0.5428\n",
      "Epoch 4/30\n",
      "65/65 [==============================] - 24s 376ms/step - loss: 1.8149 - accuracy: 0.5325 - val_loss: 1.8841 - val_accuracy: 0.4728\n",
      "Epoch 5/30\n",
      "65/65 [==============================] - 24s 375ms/step - loss: 1.7388 - accuracy: 0.5287 - val_loss: 1.7297 - val_accuracy: 0.5331\n",
      "Epoch 6/30\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 1.5564 - accuracy: 0.5855 - val_loss: 1.6656 - val_accuracy: 0.5409\n",
      "Epoch 7/30\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 1.4625 - accuracy: 0.6076 - val_loss: 1.6630 - val_accuracy: 0.5525\n",
      "Epoch 8/30\n",
      "65/65 [==============================] - 24s 377ms/step - loss: 1.4065 - accuracy: 0.6166 - val_loss: 1.6430 - val_accuracy: 0.5447\n",
      "Epoch 9/30\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 1.3259 - accuracy: 0.6580 - val_loss: 1.5957 - val_accuracy: 0.5642\n",
      "Epoch 10/30\n",
      "65/65 [==============================] - 24s 376ms/step - loss: 1.2158 - accuracy: 0.6898 - val_loss: 1.8472 - val_accuracy: 0.4825\n",
      "Epoch 11/30\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 1.2244 - accuracy: 0.6900 - val_loss: 1.7866 - val_accuracy: 0.4825\n",
      "Epoch 12/30\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 1.3661 - accuracy: 0.6055 - val_loss: 1.6378 - val_accuracy: 0.5564\n",
      "Epoch 13/30\n",
      "65/65 [==============================] - 25s 392ms/step - loss: 1.2714 - accuracy: 0.6767 - val_loss: 1.7083 - val_accuracy: 0.5661\n",
      "Epoch 14/30\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 1.2105 - accuracy: 0.7021 - val_loss: 1.7002 - val_accuracy: 0.5778\n",
      "Epoch 15/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 1.1323 - accuracy: 0.7079 - val_loss: 1.8282 - val_accuracy: 0.5447\n",
      "Epoch 16/30\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 1.1195 - accuracy: 0.7430 - val_loss: 1.6751 - val_accuracy: 0.5311\n",
      "Epoch 17/30\n",
      "65/65 [==============================] - 24s 377ms/step - loss: 1.0728 - accuracy: 0.7404 - val_loss: 1.6842 - val_accuracy: 0.5817\n",
      "Epoch 18/30\n",
      "65/65 [==============================] - 24s 376ms/step - loss: 1.0782 - accuracy: 0.7477 - val_loss: 1.8025 - val_accuracy: 0.5447\n",
      "Epoch 19/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 1.0533 - accuracy: 0.7519 - val_loss: 1.8347 - val_accuracy: 0.5447\n",
      "Epoch 20/30\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 0.9417 - accuracy: 0.7870 - val_loss: 1.7221 - val_accuracy: 0.5545\n",
      "Epoch 21/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 0.9863 - accuracy: 0.7851 - val_loss: 1.7829 - val_accuracy: 0.5233\n",
      "Epoch 22/30\n",
      "65/65 [==============================] - 24s 378ms/step - loss: 0.9442 - accuracy: 0.7889 - val_loss: 1.9674 - val_accuracy: 0.5175\n",
      "Epoch 23/30\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 1.0005 - accuracy: 0.7719 - val_loss: 1.7991 - val_accuracy: 0.5545\n",
      "Epoch 24/30\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 0.8516 - accuracy: 0.8202 - val_loss: 1.8952 - val_accuracy: 0.5506\n",
      "Epoch 25/30\n",
      "65/65 [==============================] - 24s 378ms/step - loss: 0.9387 - accuracy: 0.7967 - val_loss: 1.8834 - val_accuracy: 0.5642\n",
      "Epoch 26/30\n",
      "65/65 [==============================] - 24s 375ms/step - loss: 0.8816 - accuracy: 0.8294 - val_loss: 1.8847 - val_accuracy: 0.5136\n",
      "Epoch 27/30\n",
      "65/65 [==============================] - 24s 376ms/step - loss: 0.9655 - accuracy: 0.7742 - val_loss: 2.0028 - val_accuracy: 0.5681\n",
      "Epoch 28/30\n",
      "65/65 [==============================] - 24s 376ms/step - loss: 0.8968 - accuracy: 0.8228 - val_loss: 2.0641 - val_accuracy: 0.5370\n",
      "Epoch 29/30\n",
      "65/65 [==============================] - 24s 377ms/step - loss: 0.8494 - accuracy: 0.8308 - val_loss: 1.9147 - val_accuracy: 0.5253\n",
      "Epoch 30/30\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 0.8350 - accuracy: 0.8459 - val_loss: 1.9329 - val_accuracy: 0.5545\n",
      "35/35 [==============================] - 10s 296ms/step - loss: 2.2715 - accuracy: 0.4814\n",
      "Accuracy: 0.48138055205345154\n"
     ]
    }
   ],
   "source": [
    "convolutional_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "convolutional_model.fit(train_ds, train_labels, epochs=30, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = convolutional_model.evaluate(test_ds, test_labels)\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
