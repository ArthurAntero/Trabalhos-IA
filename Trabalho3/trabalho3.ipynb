{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3 de NoIA\n",
    "\n",
    "Neste trabalho, vamos aprender a utilizar duas técnicas diferentes de tradução automática, em que a primeira utilizará-se a estatística e, a segunda, Machine Learning na arquitetura codificador-decodificador.\n",
    "\n",
    "Para que funcione corretamente, vá ao site https://www.statmt.org/europarl/ , encontre o link “parallel corpus Portuguese-English” o baixe e crie uma pasta \"texts\" e coloque os dois arquivos que serão baixados nessa pasta \"texts \"na raiz do projeto.\n",
    "\n",
    "## Alunos\n",
    "- **Arthur de Sá Antero - 212006577**\n",
    "- **Arthur Mota Furtado - 200014935**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.translate import IBMModel1, AlignedSent\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregamento de dados\n",
    "\n",
    "Primeiro, carrega-se os textos baixados em duas matrizes, uma com tudo em português e, outra, com tudo em inglês. Cada um dos textos possuem textos do parlamento europeu nas duas línguas, tendo o mesmo conteúdo que nos ajudará a treinar um modelo de tradução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglês: [['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period'], ['although', 'as', 'you', 'will', 'have', 'seen', 'the', 'dreaded', 'millennium', 'bug', 'failed', 'to', 'materialise', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', 'during', 'this', 'partsession'], ['in', 'the', 'meantime', 'i', 'should', 'like', 'to', 'observe', 'a', 'minute', 's', 'silence', 'as', 'a', 'number', 'of', 'members', 'have', 'requested', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union']]\n",
      "Português: [['reinício', 'da', 'sessão'], ['declaro', 'reaberta', 'a', 'sessão', 'do', 'parlamento', 'europeu', 'que', 'tinha', 'sido', 'interrompida', 'na', 'sextafeira', '17', 'de', 'dezembro', 'último', 'e', 'renovo', 'todos', 'os', 'meus', 'votos', 'esperando', 'que', 'tenham', 'tido', 'boas', 'férias'], ['como', 'puderam', 'constatar', 'o', 'grande', 'bug', 'do', 'ano', '2000', 'não', 'aconteceu', 'em', 'contrapartida', 'os', 'cidadãos', 'de', 'alguns', 'dos', 'nossos', 'países', 'foram', 'vítimas', 'de', 'catástrofes', 'naturais', 'verdadeiramente', 'terríveis'], ['os', 'senhores', 'manifestaram', 'o', 'desejo', 'de', 'se', 'proceder', 'a', 'um', 'debate', 'sobre', 'o', 'assunto', 'nos', 'próximos', 'dias', 'durante', 'este', 'período', 'de', 'sessões'], ['entretanto', 'gostaria', 'como', 'também', 'me', 'foi', 'pedido', 'por', 'um', 'certo', 'número', 'de', 'colegas', 'que', 'observássemos', 'um', 'minuto', 'de', 'silêncio', 'por', 'todas', 'as', 'vítimas', 'nomeadamente', 'das', 'tempestades', 'nos', 'diferentes', 'países', 'da', 'união', 'europeia', 'que', 'foram', 'afectados']]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "file_en = \"texts/europarl-v7.pt-en.en\"\n",
    "file_pt = \"texts/europarl-v7.pt-en.pt\"\n",
    "\n",
    "with open(file_en, \"r\", encoding=\"utf-8\") as f_en, open(file_pt, \"r\", encoding=\"utf-8\") as f_pt:\n",
    "    source_sentences = f_en.readlines()  # Inglês\n",
    "    target_sentences = f_pt.readlines()  # Português\n",
    "\n",
    "source_sentences = [clean_text(line.strip()).split() for line in source_sentences[:50000]]\n",
    "target_sentences = [clean_text(line.strip()).split() for line in target_sentences[:50000]]\n",
    "\n",
    "print(\"Inglês:\", source_sentences[:5])\n",
    "print(\"Português:\", target_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 1\n",
    "\n",
    "O primeiro modelo que criaremos de tradução automática será puramente estatístico e simples chamado IBM Model 1, que basicamente aprenderá a olhar frase por frase dos dois textos que temos como dados, com a quantidade de frases que decidimos colocar para ele ler, e aprender a traduzir com base nessas duas línguas e cada frase traduzida nesses dois textos. Assim, o modelo ficará limitado a frases específicas do parlamento europeu, que pode ser um pouco formal demais e também fica limitado a uma certa quantidade de frases que decidimos que será treinado, pois pegar os dois textos inteiros demoraria muito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AlignedSent(['reinício', 'da', 'sessão'], ['resumption', 'of', 'the', 'session'], Alignment([])), AlignedSent(['declaro', 'reaberta', 'a', 'sessão', 'do', 'parlamento', 'europeu', 'que', 'tinha', 'sido', 'interrompida', 'na', 'sextafeira', '17', 'de', 'dezembro', 'último', 'e', 'renovo', 'todos', 'os', 'meus', 'votos', 'esperando', 'que', 'tenham', 'tido', 'boas', 'férias'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period'], Alignment([])), AlignedSent(['como', 'puderam', 'constatar', 'o', 'grande', 'bug', 'do', 'ano', '2000', 'não', 'aconteceu', 'em', 'contrapartida', 'os', 'cidadãos', 'de', 'alguns', 'dos', 'nossos', 'países', 'foram', 'vítimas', 'de', 'catástrofes', 'naturais', 'verdadeiramente', 'terríveis'], ['although', 'as', 'you', 'will', 'have', 'seen', 'the', 'dreaded', 'millennium', 'bug', 'failed', 'to', 'materialise', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful'], Alignment([])), AlignedSent(['os', 'senhores', 'manifestaram', 'o', 'desejo', 'de', 'se', 'proceder', 'a', 'um', 'debate', 'sobre', 'o', 'assunto', 'nos', 'próximos', 'dias', 'durante', 'este', 'período', 'de', 'sessões'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', 'during', 'this', 'partsession'], Alignment([])), AlignedSent(['entretanto', 'gostaria', 'como', 'também', 'me', 'foi', 'pedido', 'por', 'um', 'certo', 'número', 'de', 'colegas', 'que', 'observássemos', 'um', 'minuto', 'de', 'silêncio', 'por', 'todas', 'as', 'vítimas', 'nomeadamente', 'das', 'tempestades', 'nos', 'diferentes', 'países', 'da', 'união', 'europeia', 'que', 'foram', 'afectados'], ['in', 'the', 'meantime', 'i', 'should', 'like', 'to', 'observe', 'a', 'minute', 's', 'silence', 'as', 'a', 'number', 'of', 'members', 'have', 'requested', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union'], Alignment([]))]\n"
     ]
    }
   ],
   "source": [
    "bitext = [AlignedSent(target, source) for source, target in zip(source_sentences, target_sentences)]\n",
    "print(bitext[:5])\n",
    "\n",
    "ibm_model = IBMModel1(bitext, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação\n",
    "\n",
    "Para a avaliação das traduções, utilizou-se o BLEU, que pega a tradução feita e compara com a frase de referência e avalia em um valor de 0 a 1 o quão boa foi a tradução, em que 0 é que a tradução está completamente diferente da referência e 1 é que a tradução é idêntica a referência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'trial']\n",
    "score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir traduz um texto que você passa a ela utilizando o IBM Model 1 que foi feito acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_input(ibm_model, source_text):\n",
    "    translated_words = []\n",
    "    for source_word in source_text:\n",
    "        max_prob = 0.0\n",
    "        translated_word = None\n",
    "        for target_word in ibm_model.translation_table[source_word]:\n",
    "            prob = ibm_model.translation_table[source_word][target_word]\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                translated_word = target_word\n",
    "        if translated_word is not None:\n",
    "            translated_words.append(translated_word)\n",
    "\n",
    "    return translated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho a seguir de código pega 100 frases dos próprios textos em que a IBM Model 1 foi treinada, as traduz e avalia as traduções utilizando o BLEU citado acima nas avaliações, fazendo uma média aritmética de todos os \"scores\" de tradução dado pelo BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2760668194483066\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(100):\n",
    "  sentence = target_sentences[1000 + i]\n",
    "  correct_sentence = source_sentences[1000 + i]\n",
    "  translation = translate_input(ibm_model, sentence)\n",
    "  scores.append(sentence_bleu([correct_sentence], translation,weights=(1, 0, 0, 0)))\n",
    "score = sum(scores) / len(scores)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n",
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "class MTFraEng(d2l.DataModule):  #@save\n",
    "    \"\"\"The English-French dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root,\n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "data = MTFraEng()\n",
    "raw_text = data._download()\n",
    "print(raw_text[:75])\n",
    "\n",
    "class Seq2SeqEncoder(d2l.Encoder):  #@save\n",
    "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n",
    "\n",
    "    def call(self, X, *args):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        embs = self.embedding(tf.transpose(X))\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        outputs, state = self.rnn(embs)\n",
    "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, state\n",
    "\n",
    "\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "\n",
    "    def call(self, X, state):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(tf.transpose(X))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens)\n",
    "        context = enc_output[-1]\n",
    "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
    "        context = tf.tile(tf.expand_dims(context, 0), (embs.shape[0], 1, 1))\n",
    "        # Concat at the feature dimension\n",
    "        embs_and_context = tf.concat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = tf.transpose(self.dense(outputs), (1, 0, 2))\n",
    "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
    "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, [enc_output, hidden_state]\n",
    "\n",
    "class Seq2Seq(d2l.EncoderDecoder):  #@save\n",
    "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer is used here\n",
    "        return tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "    \n",
    "data = d2l.MTFraEng(batch_size=128)\n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
    "with d2l.try_gpu():\n",
    "    encoder = Seq2SeqEncoder(\n",
    "        len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "    decoder = Seq2SeqDecoder(\n",
    "        len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
    "                    lr=0.005)\n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)\n",
    "trainer.fit(model, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() missing 1 required positional argument: 'dec_X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m     model \u001b[38;5;241m=\u001b[39m Seq2Seq(encoder, decoder, tgt_pad\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mtgt_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)\n\u001b[1;32m    112\u001b[0m trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/d2l/tensorflow.py:271\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/d2l/tensorflow.py:285\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m--> 285\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip_val \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/d2l/tensorflow.py:205\u001b[0m, in \u001b[0;36mModule.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m--> 205\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, l, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: call() missing 1 required positional argument: 'dec_X'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "class MTEngPor(d2l.DataModule):  #@save\n",
    "    \"\"\"The English-Portuguese dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        file_en = \"texts/europarl-v7.pt-en.en\"\n",
    "        file_pt = \"texts/europarl-v7.pt-en.pt\"\n",
    "        \n",
    "        with open(file_en, \"r\", encoding=\"utf-8\") as f_en, open(file_pt, \"r\", encoding=\"utf-8\") as f_pt:\n",
    "            source_sentences = f_en.readlines()\n",
    "            target_sentences = f_pt.readlines()\n",
    "        \n",
    "        # Preprocess and tokenize\n",
    "        source_sentences = [clean_text(line.strip()).split() for line in source_sentences[:50000]]\n",
    "        target_sentences = [clean_text(line.strip()).split() for line in target_sentences[:50000]]\n",
    "        \n",
    "        return source_sentences, target_sentences\n",
    "    \n",
    "    def __init__(self, batch_size=128, num_steps=10):\n",
    "        super().__init__()\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "        self.src, self.tgt = self._download()\n",
    "        self.src_vocab = d2l.Vocab(self.src, min_freq=2)\n",
    "        self.tgt_vocab = d2l.Vocab(self.tgt, min_freq=2)\n",
    "    \n",
    "    def _build_arrays(self, lines, vocab, max_len):\n",
    "        arrays = []\n",
    "        for line in lines:\n",
    "            if len(line) < max_len:\n",
    "                line = line + ['<pad>'] * (max_len - len(line))  # Padding\n",
    "            arrays.append(vocab[line[:max_len]])  # Truncate if necessary\n",
    "        return arrays\n",
    "\n",
    "    \n",
    "    def get_dataloader(self, train=True):\n",
    "        src_array = self._build_arrays(self.src, self.src_vocab, self.num_steps)\n",
    "        tgt_array = self._build_arrays(self.tgt, self.tgt_vocab, self.num_steps)\n",
    "\n",
    "        # Ensure same length with padding\n",
    "        src_array = [seq + [self.src_vocab['<pad>']] * (self.num_steps - len(seq)) for seq in src_array]\n",
    "        tgt_array = [seq + [self.tgt_vocab['<pad>']] * (self.num_steps - len(seq)) for seq in tgt_array]\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        src_tensor = tf.convert_to_tensor(src_array, dtype=tf.int32)\n",
    "        tgt_tensor = tf.convert_to_tensor(tgt_array, dtype=tf.int32)\n",
    "\n",
    "        # Split into decoder input and target output\n",
    "        dec_X = tgt_tensor[:, :-1]  # All except last token\n",
    "        tgt_Y = tgt_tensor[:, 1:]   # Shifted target sequence\n",
    "\n",
    "        return d2l.load_array((src_tensor, dec_X, tgt_Y), self.batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = MTEngPor()\n",
    "\n",
    "# Model parameters\n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
    "\n",
    "class Seq2SeqEncoder(d2l.Encoder):  #@save\n",
    "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n",
    "    \n",
    "    def call(self, X, *args):\n",
    "        embs = self.embedding(tf.transpose(X))\n",
    "        outputs, state = self.rnn(embs)\n",
    "        return outputs, state\n",
    "\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"The RNN decoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "    \n",
    "    def call(self, X, state):\n",
    "        embs = self.embedding(tf.transpose(X))\n",
    "        enc_output, hidden_state = state\n",
    "        context = enc_output[-1]\n",
    "        context = tf.tile(tf.expand_dims(context, 0), (embs.shape[0], 1, 1))\n",
    "        embs_and_context = tf.concat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = tf.transpose(self.dense(outputs), (1, 0, 2))\n",
    "        return outputs, [enc_output, hidden_state]\n",
    "\n",
    "class Seq2Seq(d2l.EncoderDecoder):  #@save\n",
    "    \"\"\"The RNN encoder-decoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "# Training\n",
    "with d2l.try_gpu():\n",
    "    encoder = Seq2SeqEncoder(len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "    decoder = Seq2SeqDecoder(len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'], lr=0.005)\n",
    "    \n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)\n",
    "trainer.fit(model, data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
